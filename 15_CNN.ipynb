{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "824bf581-a44e-4d15-8473-a33dd2da6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Part 0 - Remove corrupted images\n",
    "# -------------------------\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "# Image: Used to open and verify image files.\n",
    "# UnidentifiedImageError: Raised when PIL(Pillow) fails to recognize a file as an image.\n",
    "# os: Used to navigate the file system.\n",
    "\n",
    "def remove_corrupted_images(folder_path):   #Defines a function called remove_corrupted_images that takes a folder path as input.\n",
    "    deleted = 0\n",
    "    for root, _, files in os.walk(folder_path):                            # os.walk() recursively walks through the folder and all its subfolders.root: current directory path._: list of subdirectories (not used here, so it’s _).files: list of files in the current root.\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):         #Checks if the file is an image based on its extension (case-insensitive).Only processes supported image types.\n",
    "                file_path = os.path.join(root, file)         #Joins the folder path and file name to get the full path to the image.\n",
    "                try:                            #Tries to:Open the image using Image.open().Call .verify() to ensure the image is not corrupted (doesn’t load pixels, just checks header and structure).\n",
    "                    img = Image.open(file_path)             \n",
    "                    img.verify()  # Verify image file integrity\n",
    "                except (IOError, UnidentifiedImageError, SyntaxError) as e:           #If the image is unreadable or corrupted, it may raise:IOError: generic file read error.UnidentifiedImageError: image file can’t be opened as an image.SyntaxError: malformed image (older PIL versions sometimes throw this).\n",
    "                    print(f\"❌ Deleting corrupted file: {file_path} — {e}\")            #Prints which file is being deleted and why.\n",
    "                    os.remove(file_path)         #Deletes the corrupted image file.\n",
    "                    deleted += 1     #Increments the deleted counter.\n",
    "    print(f\"\\n✅ Done. Total corrupted images removed from {folder_path}: {deleted}\")        #After the loop, prints a summary of how many images were removed from the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2aefa8c-6954-4651-ac2e-eef2ca92143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Total corrupted images removed from cnnData/training_set: 0\n",
      "\n",
      "✅ Done. Total corrupted images removed from cnnData/test_set: 0\n"
     ]
    }
   ],
   "source": [
    "# Clean corrupted images before training\n",
    "remove_corrupted_images(\"cnnData/training_set\")\n",
    "remove_corrupted_images(\"cnnData/test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ab7f165-fccc-420a-814a-de555a4b86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Part 1 - CNN Setup & Preprocessing\n",
    "# -------------------------\n",
    "import tensorflow as tf         #TensorFlow is a deep learning framework used for building and training models like CNNs.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator         #Imports ImageDataGenerator, which is a powerful tool to:Load images from folders,Augment images (rotate, flip, zoom, etc.),Normalize pixel values (like rescaling to 0–1).\n",
    "import numpy as np                  #It’s used later to manipulate image arrays (e.g. np.expand_dims to convert an image into a batch of size 1).\n",
    "from keras.preprocessing import image              #Imports the image module from Keras to help- Load and preprocess single images,Convert images to arrays for prediction using image.load_img() and image.img_to_array().\n",
    "print(\"✅ TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58514fb7-9f98-4df3-81c2-80e36d8b2842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6501 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Training Set\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# rescale=1./255: Normalizes image pixel values from 0–255 to 0–1, which helps neural networks converge faster.\n",
    "# shear_range=0.2: Randomly shears (distorts) the image along an axis, making the model more robust to skewed perspectives.\n",
    "# zoom_range=0.2: Randomly zooms into images, helping the model learn better scale invariance.\n",
    "# horizontal_flip=True: Randomly flips images left-to-right — useful for symmetrical objects (e.g., cats/dogs facing either way).\n",
    "                                                                  \n",
    "training_set = train_datagen.flow_from_directory('cnnData/training_set',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "# flow_from_directory(...): Loads images from folders on disk.\n",
    "# 'cnnData/training_set':Folder should contain subfolders like /cats, /dogs.Each subfolder name becomes a class label.\n",
    "# target_size=(64, 64):Resizes every image to 64×64 pixels before feeding to the model.Your CNN is built to accept input shape (64, 64, 3).\n",
    "# batch_size=32:Loads and processes 32 images at a time (faster & memory-efficient).\n",
    "# class_mode='binary':Used for 2 classes only (e.g., cat vs dog).Labels are 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a0b402e-4667-431e-863c-0c26997af92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 143 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Test Set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Creates a test_datagen object to handle test image preprocessing.\n",
    "# rescale=1./255:Normalizes image pixel values from [0–255] to [0–1].\n",
    "# This is essential to match the training image format. No augmentations (like flip/zoom) are applied to test data — test data must reflect real-world, untouched inputs.\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('cnnData/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "# Loads and preprocesses test images from 'cnnData/test_set'.\n",
    "# target_size=(64, 64):Resizes all test images to 64×64 pixels to match the CNN input shape.\n",
    "# batch_size=32:Loads 32 images at a time during validation.\n",
    "# class_mode='binary':Labels are binary: 0 or 1, suitable for 2-class classification (e.g., cats vs dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faaf56ed-f09c-4834-993a-c565a239382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Part 2 - Build the CNN\n",
    "# -------------------------\n",
    "cnn = tf.keras.models.Sequential([              #Initializes a Sequential model, meaning layers will be stacked one after another.\n",
    "    tf.keras.Input(shape=(64, 64, 3)),           #Defines the input shape:64 x 64 pixels (as per preprocessing).3 channels → for RGB images.This is the shape of each image entering the model.\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),           #Applies 32 filters of size 3×3 to the input image.Captures features like edges, textures, etc.activation='relu': Applies the ReLU function, introducing non-linearity.\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),                #Max Pooling layer:Reduces image size by taking the max value in each 2×2 window.Helps reduce computation and overfitting by downsampling.\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),                \n",
    "    #Another Conv + MaxPooling block:Deepens the model and allows it to learn more complex patterns.Still uses 32 filters of size 3×3 and same downsampling logic.\n",
    "    tf.keras.layers.Flatten(),          #Converts the 2D feature maps from the last layer into a 1D vector.This is needed to pass it to the fully connected (dense) layers.\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),    \n",
    "    #Fully connected hidden layer:128 neurons (a common starting point).Uses ReLU activation to learn complex patterns from extracted features.\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')  #Output layer for binary classification:1 neuron (since only 2 classes: e.g., cat vs dog).Sigmoid activation gives output between 0 and 1 → interpreted as class probability.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47a64e62-e981-4637-80c1-a9fe964d336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 298ms/step - accuracy: 0.5877 - loss: 0.6696 - val_accuracy: 0.5594 - val_loss: 0.6788\n",
      "Epoch 2/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 335ms/step - accuracy: 0.6656 - loss: 0.6049 - val_accuracy: 0.6503 - val_loss: 0.6051\n",
      "Epoch 3/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 353ms/step - accuracy: 0.7006 - loss: 0.5597 - val_accuracy: 0.7063 - val_loss: 0.5290\n",
      "Epoch 4/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 328ms/step - accuracy: 0.7198 - loss: 0.5481 - val_accuracy: 0.7483 - val_loss: 0.5247\n",
      "Epoch 5/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 290ms/step - accuracy: 0.7308 - loss: 0.5238 - val_accuracy: 0.7483 - val_loss: 0.4508\n",
      "Epoch 6/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 270ms/step - accuracy: 0.7570 - loss: 0.4941 - val_accuracy: 0.7552 - val_loss: 0.4928\n",
      "Epoch 7/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 327ms/step - accuracy: 0.7775 - loss: 0.4649 - val_accuracy: 0.7902 - val_loss: 0.4328\n",
      "Epoch 8/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 272ms/step - accuracy: 0.7828 - loss: 0.4582 - val_accuracy: 0.8252 - val_loss: 0.3897\n",
      "Epoch 9/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 277ms/step - accuracy: 0.7909 - loss: 0.4453 - val_accuracy: 0.7902 - val_loss: 0.4553\n",
      "Epoch 10/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 267ms/step - accuracy: 0.8062 - loss: 0.4259 - val_accuracy: 0.8811 - val_loss: 0.3095\n",
      "Epoch 11/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 262ms/step - accuracy: 0.8164 - loss: 0.3984 - val_accuracy: 0.8322 - val_loss: 0.3381\n",
      "Epoch 12/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - accuracy: 0.8131 - loss: 0.4016 - val_accuracy: 0.8741 - val_loss: 0.2851\n",
      "Epoch 13/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 272ms/step - accuracy: 0.8286 - loss: 0.3831 - val_accuracy: 0.8182 - val_loss: 0.3639\n",
      "Epoch 14/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 274ms/step - accuracy: 0.8305 - loss: 0.3727 - val_accuracy: 0.8951 - val_loss: 0.3069\n",
      "Epoch 15/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 262ms/step - accuracy: 0.8462 - loss: 0.3516 - val_accuracy: 0.9231 - val_loss: 0.2596\n",
      "Epoch 16/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 280ms/step - accuracy: 0.8537 - loss: 0.3302 - val_accuracy: 0.8462 - val_loss: 0.3199\n",
      "Epoch 17/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 383ms/step - accuracy: 0.8545 - loss: 0.3254 - val_accuracy: 0.8951 - val_loss: 0.2305\n",
      "Epoch 18/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 321ms/step - accuracy: 0.8616 - loss: 0.3187 - val_accuracy: 0.9021 - val_loss: 0.2297\n",
      "Epoch 19/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 393ms/step - accuracy: 0.8634 - loss: 0.3090 - val_accuracy: 0.8741 - val_loss: 0.3510\n",
      "Epoch 20/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 371ms/step - accuracy: 0.8739 - loss: 0.2893 - val_accuracy: 0.9301 - val_loss: 0.1845\n",
      "Epoch 21/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 321ms/step - accuracy: 0.8831 - loss: 0.2714 - val_accuracy: 0.9021 - val_loss: 0.2298\n",
      "Epoch 22/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 351ms/step - accuracy: 0.8918 - loss: 0.2555 - val_accuracy: 0.9231 - val_loss: 0.2158\n",
      "Epoch 23/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 364ms/step - accuracy: 0.8936 - loss: 0.2509 - val_accuracy: 0.9720 - val_loss: 0.1180\n",
      "Epoch 24/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 362ms/step - accuracy: 0.9019 - loss: 0.2428 - val_accuracy: 0.9371 - val_loss: 0.2021\n",
      "Epoch 25/25\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 373ms/step - accuracy: 0.9023 - loss: 0.2278 - val_accuracy: 0.8951 - val_loss: 0.2364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2925a8400b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Part 3 - Compile & Train\n",
    "# -------------------------\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# This line prepares the model for training by setting:\n",
    "#optimizer='adam'\n",
    "# Uses the Adam optimizer (Adaptive Moment Estimation).\n",
    "# Combines benefits of SGD + RMSProp.\n",
    "# Automatically adjusts the learning rate → works well in most deep learning tasks.\n",
    "# 🔹 loss='binary_crossentropy'\n",
    "# Suitable for binary classification (two output classes: e.g., cats vs dogs).\n",
    "# Measures how far the predicted probability is from the true label (0 or 1).\n",
    "# 🔹 metrics=['accuracy']\n",
    "# Tracks accuracy during training and validation.\n",
    "# Accuracy = % of correct predictions.\n",
    "\n",
    "cnn.fit(x=training_set, validation_data=test_set, epochs=25)\n",
    "#This line starts the training of your CNN model.\n",
    "# 🔹 x=training_set\n",
    "# The training data generator you created earlier.\n",
    "# Loads and feeds batches of images and labels to the model.\n",
    "# 🔹 validation_data=test_set\n",
    "# Validation data generator.\n",
    "# Model evaluates its performance on this set after each epoch (to monitor generalization).\n",
    "# 🔹 epochs=25\n",
    "# The number of times the model will see the entire training dataset.\n",
    "# More epochs can improve accuracy but risk overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "008b004d-4263-4857-b57a-6469afb3ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy - How well the model is doing on the training data. E.g. 0.9023 means 90.23% correct on the training set.\n",
    "# loss - A number that shows how bad the model's predictions are on training data. Lower is better.\n",
    "# val_accuracy - Accuracy on the test (validation) data — how well it generalizes to unseen data.\n",
    "# val_loss - Loss on the test/validation data — again, lower is better.\n",
    "\n",
    "# Training and validation accuracy are both above 90%.\n",
    "# Validation loss is low and stable, meaning it's not overfitting.\n",
    "# The gap between accuracy and val_accuracy is small — your model generalizes well.\n",
    "# You're not seeing signs of overfitting (e.g., high training acc but low val acc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d6cff86-736b-42df-83af-6518f1895186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Raw output: [[1.]]\n",
      "Predicted class: dog\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Part 4 - Single Prediction\n",
    "# -------------------------\n",
    "img_path = os.path.join('cnnData', 'prediction', 'cat.69.jpg')     #Builds the path to your image: cnnData/prediction/cat.69.jpg.Using os.path.join makes it platform-safe (Windows/Linux).\n",
    "\n",
    "try:\n",
    "    test_image = image.load_img(img_path, target_size=(64, 64))          #Loads the image and resizes it to the expected model input size (64x64 pixels).\n",
    "    test_image = image.img_to_array(test_image)           #Converts the image to a NumPy array (height × width × channels).\n",
    "    test_image = np.expand_dims(test_image, axis=0)          \n",
    "    #Adds an extra dimension → converts shape from (64, 64, 3) to (1, 64, 64, 3).This mimics a batch of size 1, required by model.predict().\n",
    "    #When you trained your CNN, you gave it batches of images shaped like:(batch_size, height, width, channels)→ e.g. (32, 64, 64, 3)\n",
    "    #Even if you want to predict just one image, the model still expects a batch, not a single image.  So you convert:(64, 64, 3) → (1, 64, 64, 3)\n",
    "\n",
    "    result = cnn.predict(test_image)   #Predicts the class probability using the CNN.\n",
    "\n",
    "    print(\"Raw output:\", result)  #Shows the raw model output (e.g. [[0.86]]).\n",
    "\n",
    "    if result[0][0] >= 0.5:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "\n",
    "    #If the output is ≥ 0.5, it's considered class 1 (dog).Otherwise, class 0 (cat).TensorFlow assigns labels based on directory names in alphabetical order:So by default:cat = 0, dog = 1\n",
    "\n",
    "    print(\"Predicted class:\", prediction)\n",
    "\n",
    "except (UnidentifiedImageError, IOError) as e:\n",
    "    print(f\"❌ Could not read prediction image: {img_path} — {e}\")      #If the image is corrupted or unreadable, it shows a helpful error message instead of crashing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55d4d6-f7d6-4bde-936b-db2138be64a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
